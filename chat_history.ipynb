{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2LikTJPvTxj8Az5+UszAv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shashidhar-Gowda/ChatBot/blob/mlds/chat_history.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bxn_kIs0BqT2"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.runnables import RunnableSequence\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_core.chat_history import InMemoryChatMessageHistory\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -qU \"langchain[groq]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DpwJJJmBsQF",
        "outputId": "5f8ecef0-282e-4dd6-9583-6978272cdcdf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/423.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m419.8/423.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.3/423.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.7/126.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if not os.environ.get(\"GROQ_API_KEY\"):\n",
        "  os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter API key for Groq: \")\n",
        "\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "model = init_chat_model(\"deepseek-r1-distill-llama-70b\", model_provider=\"groq\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2jQ4YeSCVSW",
        "outputId": "cb0d9a04-9810-425f-a651-82921064e2c9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter API key for Groq: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant.\"),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n"
      ],
      "metadata": {
        "id": "jooM_Un8CbFv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | model\n"
      ],
      "metadata": {
        "id": "LS-2KLqnCgEC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_with_history = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    lambda session_id: InMemoryChatMessageHistory(),  # you can replace this with Redis/SQlite\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"history\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "NS3lJUE1Cnyi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate session id (could be user_id or UUID)\n",
        "session_id = \"user123\"\n",
        "\n",
        "response1 = chat_with_history.invoke(\n",
        "    {\"input\": \"What is linear regression?\"},\n",
        "    config={\"configurable\": {\"session_id\": session_id}}\n",
        ")\n",
        "\n",
        "print(response1.content)\n",
        "\n",
        "response2 = chat_with_history.invoke(\n",
        "    {\"input\": \"How is R-squared used in it?\"},\n",
        "    config={\"configurable\": {\"session_id\": session_id}}\n",
        ")\n",
        "\n",
        "print(response2.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spkq41m2CqzG",
        "outputId": "6e65d468-4c7f-4072-a059-e47307394add"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<think>\n",
            "Okay, so I need to understand what linear regression is. I've heard the term before in my stats class, but I'm a bit fuzzy on the details. Let me start by breaking down the words. \"Regression\" in general, I think, has something to do with relationships between variables. \"Linear\" makes me think of a straight line. So maybe linear regression is about finding a straight line that best describes the relationship between variables.\n",
            "\n",
            "Wait, but how does that work? I remember something about predicting one variable based on another. So if I have two variables, like height and weight, maybe I can use a person's height to predict their weight. That sounds like a regression problem.\n",
            "\n",
            "I think linear regression is a statistical method. It probably involves some kind of equation. Oh, right, the equation of a line is y = mx + b, where m is the slope and b is the y-intercept. So in linear regression, we're trying to find the best-fitting line that minimizes the differences between the observed data points and the line.\n",
            "\n",
            "But how exactly do we find that best-fitting line? I remember something about least squares. Maybe that's the method used to determine the slope and intercept. Least squares minimizes the sum of the squared differences between the observed y-values and the predicted y-values. That makes sense because squaring removes any negative signs and gives more weight to larger differences.\n",
            "\n",
            "So, in simple terms, linear regression is about drawing a straight line through data points that best represents the relationship between two variables. It's used for predicting outcomes based on one or more predictors. I think there are different types, like simple linear regression with one predictor and multiple linear regression with more than one.\n",
            "\n",
            "Wait, but I'm not sure about the assumptions of linear regression. I think there are some conditions that need to be met for it to work properly. Maybe things like linearity, independence of observations, homoscedasticity (constant variance), normality of errors, and no multicollinearity. If these aren't met, the model might not be accurate or reliable.\n",
            "\n",
            "I also recall something about coefficients in the regression equation. Each coefficient represents the change in the dependent variable for a one-unit change in the independent variable, holding all other variables constant. That's important for interpretation.\n",
            "\n",
            "I'm a bit confused about overfitting in linear regression. I think it's when the model is too complex and fits the training data too well, but doesn't generalize to new data. But wait, isn't linear regression usually less prone to overfitting compared to models with more flexibility, like polynomial regression? Or maybe that's when you have too many variables relative to the data points.\n",
            "\n",
            "Another thing I'm not clear on is how to evaluate the goodness of fit. I think R-squared is a measure that tells us how well the model explains the variance in the data. A higher R-squared means a better fit. But I also heard that adjusted R-squared is better when comparing models with different numbers of predictors because it penalizes for adding unnecessary variables.\n",
            "\n",
            "I remember something about residual plots too. After fitting the model, you can plot the residuals against the predicted values to check if the assumptions hold. If the residuals form a random scatter around zero, that's good. If there's a pattern, it might indicate non-linearity or heteroscedasticity.\n",
            "\n",
            "So, putting it all together, linear regression is a method to model the relationship between variables by fitting a linear equation. It's widely used for prediction and inference. The process involves selecting the model, estimating the coefficients, checking assumptions, and evaluating the model's performance.\n",
            "\n",
            "I think I need to solidify my understanding by going through an example. Let's say I want to predict house prices based on their size. I collect data on house size (x) and price (y). I can plot these points and see if they roughly form a straight line. Then, I'd apply linear regression to find the best line that predicts y based on x. The equation would give me a way to estimate the price of a house based on its size.\n",
            "\n",
            "But wait, what if the relationship isn't linear? Then a simple linear regression might not be the best choice. I might need to consider a non-linear model or transform the variables. Or maybe there are other variables at play that I haven't considered, which could affect the model's accuracy.\n",
            "\n",
            "I should also think about the limitations. Linear regression assumes a linear relationship, so if the data has a curved relationship, the model won't capture that. It's also sensitive to outliers, which can pull the regression line away from the true relationship. And if there's multicollinearity among predictors, it can make the coefficients unstable.\n",
            "\n",
            "In summary, linear regression is a fundamental statistical technique for modeling linear relationships. It's essential to understand the underlying assumptions and to validate the model through diagnostic checks. Despite its simplicity, it's a powerful tool for both prediction and understanding the relationships between variables.\n",
            "</think>\n",
            "\n",
            "Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables by fitting a linear equation. It is primarily used for prediction and forecasting. The simplest form, simple linear regression, involves one independent variable, while multiple linear regression includes more than one independent variable.\n",
            "\n",
            "### Key Concepts:\n",
            "\n",
            "1. **Linear Equation**: The model is based on the equation \\( y = \\beta_0 + \\beta_1x + \\epsilon \\), where \\( \\beta_0 \\) is the y-intercept, \\( \\beta_1 \\) is the slope coefficient, and \\( \\epsilon \\) is the error term.\n",
            "\n",
            "2. **Ordinary Least Squares (OLS)**: This method minimizes the sum of the squared differences between observed and predicted values to estimate the coefficients \\( \\beta_0 \\) and \\( \\beta_1 \\).\n",
            "\n",
            "3. **Assumptions**:\n",
            "   - **Linearity**: The relationship between variables should be linear.\n",
            "   - **Independence**: Observations should be independent.\n",
            "   - **Homoscedasticity**: The variance of errors should be constant.\n",
            "   - **Normality**: Errors should be normally distributed.\n",
            "   - **No Multicollinearity**: Independent variables should not be highly correlated.\n",
            "\n",
            "4. **Evaluation**:\n",
            "   - **R-squared (R²)**: Measures how well the model explains the variance in the data. A higher R² indicates a better fit.\n",
            "   - **Adjusted R-squared**: Adjusts R² for the number of predictors, penalizing for unnecessary variables.\n",
            "   - **Residual Plots**: Used to check for patterns that may indicate violations of assumptions.\n",
            "\n",
            "5. **Limitations**:\n",
            "   - Assumes a linear relationship; non-linear relationships require different models.\n",
            "   - Sensitive to outliers and multicollinearity, which can affect coefficient stability.\n",
            "\n",
            "6. **Applications**: Widely used in fields like economics, engineering, and social sciences for prediction and understanding variable relationships.\n",
            "\n",
            "In summary, linear regression is a powerful tool for modeling linear relationships, requiring careful validation to ensure accurate and reliable results.\n",
            "<think>\n",
            "Okay, so I'm trying to understand how R-squared is used in regression analysis. I remember from my stats class that R-squared, or the coefficient of determination, has something to do with how well the model explains the data. But I'm a bit fuzzy on the details. Let me think through this step by step.\n",
            "\n",
            "First, what is R-squared exactly? I think it's a measure that tells us the proportion of variance in the dependent variable that's explained by the independent variables in the model. So, if I have a regression model predicting something like house prices based on features like number of bedrooms or square footage, R-squared would tell me how much of the variation in house prices is explained by those features.\n",
            "\n",
            "Wait, how is R-squared calculated? I recall it's related to the ratio of the explained variation to the total variation. So, maybe it's the sum of squares of the model divided by the total sum of squares. That makes sense because sum of squares is a measure of variability. So, R-squared = SSR / SST, where SSR is the sum of squares regression and SST is the total sum of squares.\n",
            "\n",
            "But what does that actually mean in practice? If R-squared is 0.8, for example, that means 80% of the variance is explained by the model. So, the higher the R-squared, the better the model explains the data. But I also remember that R-squared can be misleading. For instance, adding more variables to the model can artificially increase R-squared, even if those variables aren't really relevant. That's where adjusted R-squared comes in, right? It penalizes for adding unnecessary variables to give a more accurate picture of the model's explanatory power.\n",
            "\n",
            "I'm also trying to remember if R-squared has any limitations. I think it doesn't indicate whether the model is causative, just that there's a correlation. So, even with a high R-squared, the model might not be capturing the true causal relationships. Also, non-linear relationships might not be reflected properly in a linear model's R-squared. If the relationship is curved but the model is linear, the R-squared might be low even if the variables are related.\n",
            "\n",
            "Another point is overfitting. If a model is overfit, it might have a high R-squared for the training data but perform poorly on new data. So, R-squared alone isn't enough to determine the model's generalizability. We might need to look at other metrics or use techniques like cross-validation.\n",
            "\n",
            "I'm also curious about how R-squared is used in model selection. When comparing two models, the one with a higher R-squared is often preferred because it explains more variance. But again, adjusted R-squared is better for this purpose since it accounts for the number of predictors. So, in model selection, we might look at both R-squared and adjusted R-squared to balance model fit and complexity.\n",
            "\n",
            "How about interpreting R-squared in context? For example, in social sciences, an R-squared of 0.2 might be considered decent because human behavior can be influenced by many unpredictable factors. In engineering, where systems are more controlled, a much higher R-squared might be expected. So, context matters a lot in interpreting what's a \"good\" R-squared.\n",
            "\n",
            "I'm also trying to remember how R-squared relates to other statistical measures. For example, in a simple linear regression with one predictor, R-squared is the square of the correlation coefficient between the predictor and the outcome. That ties back to Pearson's r, which measures the strength and direction of a linear relationship between two variables.\n",
            "\n",
            "What about in multiple regression? With multiple predictors, R-squared still represents the proportion of variance explained, but it's based on all the predictors together. So, it's an overall measure of how well the combination of predictors explains the outcome variable.\n",
            "\n",
            "I'm a bit confused about the difference between R-squared and adjusted R-squared. I think adjusted R-squared is like a corrected version that accounts for the number of predictors in the model. It uses the degrees of freedom, so it doesn't just keep increasing with more variables added. That makes it more useful for comparing models with different numbers of predictors because it adjusts for overfitting.\n",
            "\n",
            "Wait, how is adjusted R-squared calculated? I think it's something like:\n",
            "\n",
            "Adjusted R-squared = 1 - ( (n - 1)/(n - k - 1) ) * (1 - R-squared)\n",
            "\n",
            "Where n is the sample size and k is the number of predictors. So, if you add a predictor that doesn't improve the model, the adjusted R-squared might actually decrease because the penalty for adding an extra variable outweighs the slight increase in R-squared.\n",
            "\n",
            "Another thing I'm recalling is that R-squared can be used to assess the goodness of fit of the model. A higher R-squared indicates a better fit, but as I thought earlier, it doesn't necessarily mean the model is the best or most accurate. It's just one piece of the puzzle.\n",
            "\n",
            "I also remember that in polynomial regression or when using interaction terms, R-squared can increase, but it might not always lead to a better model. It could just be overfitting to the noise in the data. So, it's important to use cross-validation or other techniques to validate the model's performance beyond just looking at R-squared.\n",
            "\n",
            "I'm trying to think of an example. Suppose I have data on exam scores and hours studied. A simple regression model might have an R-squared of 0.6, meaning 60% of the variance in exam scores is explained by hours studied. If I add another variable, like attendance, the R-squared might go up to 0.7, indicating that together, hours studied and attendance explain 70% of the variance. The adjusted R-squared would adjust this value based on the number of predictors, so it might be a bit lower, say 0.65, to account for the added variable.\n",
            "\n",
            "But if I add a variable that doesn't really affect exam scores, like the color of the pen used, the R-squared might go up a little, but the adjusted R-squared might not, or it might even decrease, telling me that adding that variable didn't contribute meaningfully to the model.\n",
            "\n",
            "I also think about how R-squared is used in econometrics. For example, in forecasting models, a high R-squared is desirable because it indicates that the model accurately predicts the outcome variable based on the predictors. But economists also look at the statistical significance of each predictor and the overall model, not just the R-squared.\n",
            "\n",
            "Another point is that R-squared is sensitive to the range of the data. If the range of the predictor variables is restricted, R-squared might be lower than it would be with a broader range. So, in samples where the predictors don't vary much, the R-squared could be misleadingly low.\n",
            "\n",
            "I'm also thinking about how R-squared relates to other metrics like mean squared error (MSE) or root mean squared error (RMSE). While R-squared gives a relative measure of fit, MSE and RMSE give an absolute measure based on the error terms. So, a model could have a high R-squared but still have a high MSE if the variance in the data is large.\n",
            "\n",
            "In summary, R-squared is a useful metric in regression analysis, but it's important to understand its limitations. It tells us the proportion of variance explained by the model, but it doesn't tell us about causality, can be misleading when overfitting, and needs to be considered alongside other metrics and adjustments, like adjusted R-squared, to get a comprehensive view of the model's performance.\n",
            "\n",
            "I think I've covered the main points, but I might have missed some nuances. Maybe I should look up some examples or further explanations to solidify my understanding. But overall, R-squared is a key metric that helps assess how well a regression model explains the variance in the data, with higher values generally indicating a better fit, though not without considering other factors.\n",
            "</think>\n",
            "\n",
            "R-squared, or the coefficient of determination, is a statistical measure used in regression analysis to assess the proportion of variance in the dependent variable that is explained by the independent variables. Here's a comprehensive summary of its use and interpretation:\n",
            "\n",
            "1. **Definition and Calculation**:\n",
            "   - R-squared (R²) is calculated as the ratio of the sum of squares regression (SSR) to the total sum of squares (SST). It quantifies how well the model explains the variance in the outcome variable.\n",
            "\n",
            "2. **Interpretation**:\n",
            "   - A high R-squared value indicates that the model explains a large proportion of the variance. For example, an R-squared of 0.8 means 80% of the variance is explained.\n",
            "   - Context matters; a \"good\" R-squared varies by field. Social sciences might accept lower values, while engineering might require higher values.\n",
            "\n",
            "3. **Limitations and Considerations**:\n",
            "   - **Overfitting**: Adding unnecessary variables can inflate R-squared, so adjusted R-squared is used to penalize for model complexity.\n",
            "   - **Correlation vs. Causation**: R-squared indicates correlation, not causation.\n",
            "   - **Model Fit**: It doesn't ensure the model is correctly specified or that it generalizes well.\n",
            "\n",
            "4. **Adjusted R-squared**:\n",
            "   - Adjusted R-squared accounts for the number of predictors, providing a more accurate measure of explanatory power by penalizing for overfitting.\n",
            "\n",
            "5. **Applications and Comparisons**:\n",
            "   - Used in model selection to compare models, with higher values generally preferred.\n",
            "   - Useful in assessing goodness of fit but should be considered alongside other metrics like MSE or RMSE.\n",
            "\n",
            "6. **Examples and Context**:\n",
            "   - In simple regression, R-squared is the square of the correlation coefficient.\n",
            "   - In multiple regression, it reflects the combined effect of all predictors.\n",
            "\n",
            "7. **Additional Metrics**:\n",
            "   - Consider metrics like MSE and RMSE for absolute measures of fit.\n",
            "   - Use cross-validation to assess model generalizability beyond R-squared.\n",
            "\n",
            "In summary, R-squared is a key metric for evaluating regression models, providing insight into the model's explanatory power. However, it should be interpreted alongside other metrics and considerations to ensure a comprehensive understanding of the model's performance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FnTjcUZdCu3X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}